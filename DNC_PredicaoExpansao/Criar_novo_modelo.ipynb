{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21113,"status":"ok","timestamp":1672879282656,"user":{"displayName":"Eduardo Zanghi","userId":"15870388057949004930"},"user_tz":180},"id":"CNmNUwBSpN5J","outputId":"b03f0d49-bbdd-41ae-a7bb-efd81b536a08"},"outputs":[{"name":"stdout","output_type":"stream","text":["######################################################\n","Instalando e carregando as bibliotecas necessárias...\n","######################################################\n","\n","\n","Mounted at /content/drive\n","Bibliotecas carregadas! Etapa 1 executada com sucesso!\n"]}],"source":["#@title 1 - Importação das bibliotecas. Favor apertar [ ▶ ] abaixo\n","\n","print(\"######################################################\")\n","print(\"Instalando e carregando as bibliotecas necessárias...\")\n","print(\"######################################################\\n\\n\")\n","# Importando as bibliotecas utilizadas\n","import pandas as pd\n","import numpy as np\n","import progressbar as pb #Biblioteca para visualização da barra de progresso para processos longos \n","import warnings\n","import pickle as pkl\n","from time import time\n","import os\n","from google.colab import files\n","from google.colab import drive\n","\n","import warnings\n","\n","# Biblioteca de ML\n","## Métricas\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n","from sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score\n","\n","## Lib auxiliares\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import GridSearchCV\n","\n","## Models\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Removendo os avisos\n","warnings.filterwarnings('ignore')\n","\n","try:\n","  # Montando o diretório do Drive para leitura de arquivos do programa\n","  drive.mount('/content/drive')\n","except:\n","  print(\"É necessário permitir que este notebook acesse seus arquivos do Google Drive.\")\n","  print(\"Favor executar novamente e autorizar o acesso.\")\n","\n","else:\n","  print(\"Bibliotecas carregadas! Etapa 1 executada com sucesso!\")\n","\n","def carregaDados(dataset='dados.xlsx'):\n","  \"\"\"Função para carregar os dados\"\"\"\n","  df = pd.read_excel(dataset, names=['cod_produto', 'etapa','year', 'month', 'day','hour','minute','valor', 'id_massa'], dtype={'valor':'float'})\n","  \n","  return df\n","\n","def dataPreparation(df):\n","  \"\"\"Função para preparação dos dados para uso nas demais etapas\"\"\"\n","  #Criando nova coluna datetime\n","  df['datetime'] = pd.to_datetime(df[['year','month','day','hour','minute']], yearfirst=True)\n","\n","  # Dic. atribuindo valores referente a etapa do processo.\n","  # Critério: números formados por 'XY', onde X referência à \"Etapa do processo\" e Y à ordem dele\n","  #      X = 1 -> Etapa de Preparação de massa\n","  #      X = 2 -> Dados Moldadora\n","  #      X = 3 -> Dados Moldadora - pós passagem nos cilindro\n","  #      X = 4 e 5 -> Dados do forno - cozimento\n","  #      X = 6 -> Dados do forno - pós cozimento\n","\n","  ordem_etapas = {'A': 1, 'B': 1, 'C': 1, 'D': 1, 'E': 1, 'F': 1, 'G': 1, 'H': 1, 'I': 1, 'J': 1, 'K': 1, 'L': 1, \n","              'S.A Farinha de trigo comum doce': 2, 'Quantidade de Fermento': 11, 'Tempo de Batimento 1ª Fase': 12, \n","              'Tempo de Batimento 2ª Fase': 13, 'Temperatura de massa': 14, 'VELOCIDADE FORÇADOR': 21, \n","              'VELOCIDADE DA MOLDADORA': 22, 'VELOCIDADE LONA MOLDADORA': 23, 'PRESSÃO ROLO ESTRIADO': 24, \n","              'PRESSÃO ROLO DE BORRACHA ESQUERDO': 25, 'PRESSÃO ROLO DE BORRACHA DIREITO': 26, 'ALTURA DA FACA': 27, 'VELOCIDADE LONA DE BANDEJA OSCILANTE': 28, \n","              'Peso cru': 31, 'Peso cru 1': 31, '[TEMPERATURA ZONA 1]': 41, 'Pressão do teto - zona 1': 42, \n","              'Pressão do lastro - zona 1': 43, '[TEMPERATURA ZONA 2]': 44, 'Pressão do teto - zona 2': 45, \n","              'Pressão do lastro - zona 2': 46, '[TEMPERATURA ZONA 3]': 47, 'Pressão do teto - zona 3': 48, 'Pressão do lastro - zona 3': 49, \n","              '[TEMPERATURA ZONA 4]': 50, 'Pressão do teto - zona 4': 51, 'Pressão do lastro - zona 4': 52, 'Tempo de cozimento': 53, \n","              'Largura': 61, 'Largura 1': 61, 'Comprimento': 62, 'Comprimento 1': 62, 'pH': 63, 'Peso assado': 64, 'Peso assado 1': 64, \n","              'Cor Hunter L - Teto': 65, 'Umidade (água)': 66, 'Expansão': 67, 'Expansão 1': 67}\n","\n","  # Criando nova coluna baseado no dicionário ordem etapas criado.\n","  df['ordem'] = df['etapa'].apply(lambda x : ordem_etapas[x])\n","\n","  # Ordenando o dataframe pelo valor data/hora\n","  df.sort_values(['datetime', 'ordem'], inplace=True)\n","\n","  # Resetando o index\n","  df.reset_index(drop=True, inplace=True)\n","\n","  #Removendo o espaço no nome das etapas\n","  df['etapa'] = df['etapa'].str.lower()\n","  df['etapa'] = df['etapa'].str.replace(\" \", \"_\")\n","  df['etapa'] = df['etapa'].str.replace(\"[\", \"\")\n","  df['etapa'] = df['etapa'].str.replace(\"]\", \"\")\n","\n","  return df\n","\n","def splitData(df):  \n","  \"\"\"Função para adicionar colunas necessárias para enriquecimento dos dados\"\"\"\n","  # Separando DataFrame Processos e Laudo\n","  ## Dataframe com os dados referente aos Laudos da massa\n","  df_laudo = df[df['ordem'] == 1].copy()\n","\n","  ## Dataframe com os dados referente aos Processos\n","  df_processo = df.loc[df['ordem'] != 1].copy()\n","  df_processo.sort_values(['datetime'], inplace=True)\n","\n","  # Preenchendo a coluna id_massa com o valor de S.A Farinha de trigo comum doce\n","  ## Os valores referente a 'S.A Farinha de trigo comum doce', indicam que aquela massa foi inserida na linha de produção àquela hora. \n","  ## Dessa forma, podemos preencher a coluna id_massa com os mesmos valores de 'S.A Farinha de trigo comum doce', para todas as etapas do horário que ele foi inserido, até o próximo registro válido.\n","  df_processo['id_massa'] = df_processo['valor'][df_processo['ordem'] == 2]\n","\n","  #Preenchendo os valores nulos com o último valor válido\n","  ## Como os valores estão ordenado por processo e data/hora, e o id_massa é o primeiro a ser registrado no início do processo, logo podemos preencher todos os valores seguintes com o mesmo id_massa até que um próximo id_massa seja inserido na linha.\n","  df_processo['id_massa'].fillna(method='ffill', inplace=True)\n","\n","  return df_processo, df_laudo\n","\n","def dataCleaningProcess(df_processo):\n","  \"\"\"Função para limpar/transformar dados do dataset de Processos\"\"\"\n","  #Removendo as linhas etapa 'S.A Farinha de trigo comum doce'\n","  df_processo = df_processo.loc[df_processo['ordem'] != 2]\n","  # Removendo colunas desnecessárias \n","  df_processo.drop(labels=['year','month','day','hour','minute'], axis=1, inplace=True)\n","  #Removendo os valores nulos\n","  df_processo.dropna(inplace=True)\n","  # Transformando a coluna id_massa para o tipo int\n","  df_processo['id_massa'] = df_processo['id_massa'].astype(int)\n","\n","  return df_processo\n","\n","def dataCleaningLaudo(df_laudo):\n","  \"\"\"Função para limpar/transformar dados do dataset de Laudo\"\"\"\n","  # Removendo \"SA ' dos valores de S.A\n","  df_laudo['id_massa_temp'] = df_laudo['id_massa'].str.strip('SAsa ')\n","\n","  # Recuperando os valores perdidos no lstrip\n","  df_laudo['id_massa_temp'].fillna(df_laudo['id_massa'], inplace=True)\n","\n","  # Copiando os valores para a coluna id_massa\n","  df_laudo['id_massa'] = df_laudo['id_massa_temp'].astype('int').copy()\n","\n","  # Removendo colunas desnecessárias \n","  df_laudo.drop(labels=['year','month','day','hour','minute', 'id_massa_temp', 'datetime'], axis=1, inplace=True)\n","\n","  # Agrupando os valores de Laudo por id_massa\n","  df_laudo = df_laudo.groupby(['id_massa', 'etapa']).mean().reset_index().copy()\n","\n","  # Pivotando os valores agrupados\n","  df_laudo = pd.pivot(df_laudo, index=['id_massa', 'cod_produto'], columns=['etapa'], values='valor').reset_index()\n","\n","  # #Preenchendo valores para a coluna L conforme fórmula\n","  df_laudo['l'] = round((df_laudo['k'] / df_laudo['a']), 3)\n","\n","  # # Lista das colunas de interesse\n","  prop_laudo = ['a', 'b', 'c', 'd', 'e','f', 'g', 'h', 'i', 'j', 'k', 'l']\n","\n","  # Retornando o df_laudo originário com as propriedades na coluna etapa\n","  df_laudo = df_laudo.melt(id_vars=['id_massa'], value_vars=prop_laudo, value_name='valor')\n","\n","  # Renomeando a coluna conforme padrão\n","  df_laudo.rename(columns={\"variable\": \"etapa\"}, inplace=True)\n","\n","  return df_laudo\n","\n","def prepareDataFeature(df_processo, df_laudo):\n","  \"\"\"Função que prepara os dados de processo (features) para a separação de amostras\"\"\"\n","  # Remover os dados de df_processo que possuem id_massa inexistente em df_laudo\n","  id_massa_unicos = list(df_laudo['id_massa'])\n","  df_feature = df_processo[df_processo['id_massa'].isin(id_massa_unicos)]\n","\n","  # Separando as colunas de interesse\n","  df_feature = df_feature[['id_massa', 'etapa', 'valor', 'ordem', 'datetime']].loc[df_feature['ordem'] != 67]\n","\n","  # agrupar dados de lados diferentes da esteira numa mesma iteração pela média\n","  df_feature = df_feature.groupby(['ordem','datetime']).mean().reset_index().sort_values(['datetime','ordem'])\n","\n","  return df_feature\n","\n","def prepareDataTarget(df_processo):\n","  \"\"\"Função que prepara os dados de expansão (variável target) para a separação de amostras\"\"\"\n","  # Seprando as colunas de interesse\n","  df_target = df_processo[['id_massa', 'etapa', 'valor', 'ordem', 'datetime']].loc[df_processo['ordem'] == 67]\n","\n","  # Agrupamento dos valores de expansão numa mesma iteração pela média\n","  df_target = df_target.groupby(['datetime']).mean().reset_index()\n","  \n","  return df_target\n","\n","def prepareEtapa():\n","  \"\"\"Função que prepara um dataframe com as etapas para a separação de amostras\"\"\"\n","\n","  ordem_etapas = {'quantidade_de_fermento': 11, 'tempo_de_batimento_1ª_fase': 12, 'tempo_de_batimento_2ª_fase': 13, 'temperatura_de_massa': 14, \n","              'velocidade_forçador': 21, 'velocidade_da_moldadora': 22, 'velocidade_lona_moldadora': 23, 'pressão_rolo_estriado': 24, \n","              'pressão_rolo_de_borracha_esquerdo': 25, 'pressão_rolo_de_borracha_direito': 26, 'altura_da_faca': 27, 'velocidade_lona_de_bandeja_oscilante': 28, \n","              'peso_cru': 31, 'temperatura_zona_1': 41, 'pressão_do_teto_-_zona_1': 42, 'pressão_do_lastro_-_zona_1': 43, 'temperatura_zona_2': 44, \n","              'pressão_do_teto_-_zona_2': 45, 'pressão_do_lastro_-_zona_2': 46, 'temperatura_zona_3': 47, 'pressão_do_teto_-_zona_3': 48, 'pressão_do_lastro_-_zona_3': 49, \n","              'temperatura_zona_4': 50, 'pressão_do_teto_-_zona_4': 51, 'pressão_do_lastro_-_zona_4': 52, 'tempo_de_cozimento': 53, \n","              'largura': 61, 'comprimento': 62, 'ph': 63, 'peso_assado': 64, 'cor_hunter_l_-_teto': 65, 'umidade_(água)': 66, 'expansão': 67}\n","\n","  # Criação do df_etapa para o algoritmo da função criada\n","  df_etapa = pd.DataFrame.from_dict(ordem_etapas, orient='index').sort_values(by=0)\n","  df_etapa.rename(columns={0:'ordem'},inplace=True)\n","  df_etapa = df_etapa.reset_index()\n","  df_etapa.rename(columns={'index':'etapa'},inplace=True)\n","\n","  #Removendo valores de ordem 1 e 2\n","  cond1 = df_etapa['ordem'] != 1\n","  cond2 = df_etapa['ordem'] != 2\n","\n","  df_etapa = df_etapa.loc[cond1 & cond2].reset_index(drop=True)\n","\n","  return df_etapa\n","\n","def prepareLaudo(df_laudo, df_target):\n","  \"\"\"Função que prepara os dados de Laudo para a separação de amostras\"\"\"\n","  # Remover os dados de df_processo que possuem id_massa inexistente em df_laudo\n","  id_massa_unicos = list(df_target['id_massa'].unique())\n","  df_laudo = df_laudo[df_laudo['id_massa'].isin(id_massa_unicos)]\n","\n","  # Separando as colunas de interesse\n","  df_laudo = df_laudo[['id_massa', 'etapa', 'valor']]\n","\n","  return df_laudo\n","\n","def get_ids(df_feature, df_etapa, df_target, df_laudo):\n","  \"\"\"Função que organiza valor de cada etapa do processo e laudo, para uma dada expansão\"\"\"\n","   # dataframe que será preenchido e retornado\n","  df_sorted = pd.DataFrame({'id_iteracao': [], 'id_massa': [], 'datetime': [], 'etapa': [], 'ordem': [], 'valor': []})\n","  id = 1\n","   \n","   # dataframe com os valores elegiveis dentro da tolerância de cada iteração para adicionar valores e pegar o mais próximo do tempo da variavel target\n","  elegiveis = pd.DataFrame({'ordem': [], 'datetime': [], 'id_massa': [], 'valor': [], 'timediff': []})\n","  # para cada linha de df_target preencher as etapas disponíveis no df_feature com base no df_etapa:\n","  for id_target, row_target in df_target.iterrows():\n","      id_massa_atual = row_target['id_massa']\n","      for id_etapa, row_etapa in df_etapa.iterrows():\n","          # se for a etapa da variável target preencher com dados da variável target\n","          if row_etapa['ordem'] == 67:\n","              df_sorted = df_sorted.append({'id_iteracao': id, 'id_massa': row_target['id_massa'], 'datetime': row_target['datetime'],\n","               'etapa': row_etapa['etapa'], 'ordem': row_etapa['ordem'], 'valor': row_target['valor']}, ignore_index=True)\n","          \n","          else:\n","              for id_feat, row_feat in df_feature.iterrows():\n","                                     \n","                  if row_feat['id_massa'] == row_target['id_massa']:\n","                      # se datetime de feature for dentro do prazo de tolerancia e se a ordem do df_etapa for a mesma que df_feature, adicionar aos elegíveis\n","                      timediff = (row_target['datetime']-row_feat['datetime']).total_seconds()/60\n","                      \n","                      if (timediff in range(0, 31) and row_etapa['ordem'] == row_feat['ordem'] and row_feat['ordem'] >= 40):\n","                          elegiveis = elegiveis.append({'ordem': row_feat['ordem'], 'datetime': row_feat['datetime'],\n","                                                      'id_massa': row_feat['id_massa'], 'valor': row_feat['valor'], 'timediff': timediff}, ignore_index=True)\n","                          df_feature.drop(index=id_feat, axis=0, inplace=True)\n","                      elif (timediff in range(25, 51) and row_etapa['ordem'] == row_feat['ordem'] and row_feat['ordem'] in range(20, 40)):\n","                          elegiveis = elegiveis.append({'ordem': row_feat['ordem'], 'datetime': row_feat['datetime'],\n","                                                      'id_massa': row_feat['id_massa'], 'valor': row_feat['valor'], 'timediff': timediff}, ignore_index=True)\n","                          df_feature.drop(index=id_feat, axis=0, inplace=True)\n","                      elif (timediff in range(35, 65) and row_etapa['ordem'] == row_feat['ordem'] and row_feat['ordem'] < 20):\n","                          elegiveis = elegiveis.append({'ordem': row_feat['ordem'], 'datetime': row_feat['datetime'],\n","                                                      'id_massa': row_feat['id_massa'], 'valor': row_feat['valor'], 'timediff': timediff}, ignore_index=True)\n","                          df_feature.drop(index=id_feat, axis=0, inplace=True)\n","              \n","              # se elegiveis for vazio, preecher df_sorted com \"-\"\n","              if len(elegiveis) == 0:\n","                  df_sorted = df_sorted.append({'id_iteracao': id, 'id_massa': np.nan, 'datetime': np.nan,\n","                                               'etapa': row_etapa['etapa'], 'ordem': row_etapa['ordem'], 'valor': np.nan}, ignore_index=True)\n","              \n","              # caso contrário preencher com o valor de df_feature com menor timediff\n","              else:\n","                  # reordenar elegiveis para pegar o menor timediff\n","                  elegiveis = elegiveis.sort_values(['timediff']).reset_index()\n","                  df_sorted = df_sorted.append({'id_iteracao': id, 'id_massa': elegiveis['id_massa'][0], 'datetime': elegiveis['datetime'][0],\n","                   'etapa': row_etapa['etapa'], 'ordem': row_etapa['ordem'], 'valor': elegiveis['valor'][0]}, ignore_index=True)\n","          # fechado uma iteração, limpar o elegiveis\n","          elegiveis = pd.DataFrame({'ordem': [], 'datetime': [], 'id_massa': [], 'valor': [], 'timediff': []})\n","      \n","      # incluir dados da massa na iteração\n","      for id_massa, row_massa in df_laudo[df_laudo['id_massa'] == row_target['id_massa']].iterrows():\n","          df_sorted = df_sorted.append({'id_iteracao': id, 'id_massa': row_target['id_massa'], 'datetime': row_target['datetime'],\n","           'etapa': row_massa['etapa'], 'ordem': row_massa['etapa'], 'valor': row_massa['valor']}, ignore_index=True)\n","      #Remover todos os valores de df_feature com id_massa diferentes\n","              \n","      if row_target['id_massa'] == id_massa_atual:\n","          # incrementar o id para a próxima iteração\n","          id += 1\n","      else:\n","          df_feature = df_feature.loc[df_feature['id_massa'] != id_massa_atual]\n","          id += 1\n","  \n","  return df_sorted\n","\n","######### FUNÇÕES PARA CRIAÇÃO DO MODELO\n","\n","def fill_values(colunas, df):\n","  '''Função para preencher todos os valores nulos nas colunas de feature do processo, onde: \n","  os primeiro são iguais ao primeiro valor existente, \n","  e os demais valores são preenchidos com a média entre os valores: [anterior + seguinte] / 2'''\n","  \n","  for col in colunas:\n","    index_Not_Null =list(df[col].loc[df[col].isna() == False].index)\n","\n","    inicial =  index_Not_Null[0]\n","    \n","    # Preenchendo todos os valores nulos com o primeiro valor preenchido\n","    if index_Not_Null[0] != 0:\n","      df[col][:inicial] = df[col].iloc[inicial]\n","\n","    # Preenchendo todos os valores nulos com a média entre o valor anterior e o próximo valor preenchido\n","    tam = len(index_Not_Null)\n","    A = 0\n","    B = 0\n","    \n","    for i in range(0, tam-1):\n","        passo = abs(index_Not_Null[i] - index_Not_Null[i+1])\n","\n","        if passo > 1:\n","          \n","            A = index_Not_Null[i]\n","            B = index_Not_Null[i+1]\n","            df[col][A+1:B] = (df[col][A] + df[col][B]) / 2\n","    \n","    # Preenchendo valores finais com o último valor preenchido\n","    df.fillna(method='ffill', inplace=True)\n","\n","def compare_models(x_train_scaled, x_test_scaled, y_train, y_test, model_list = [LinearRegression()]):\n","  \"\"\"Função que retorna o melhor modelo obtido e o modelo de Regressão Linear\"\"\"\n","  r2_rank = []\n","  model_rank = []\n","  model_dict = {}\n","\n","  for model in model_list:\n","      start = time()\n","      model.fit(x_train_scaled, y_train)\n","      train_time = time() - start\n","      start = time()\n","      y_pred = model.predict(x_test_scaled)\n","      predict_time = time()-start\n","      r2 = r2_score(y_test, y_pred)\n","      model_rank.append(model)\n","      r2_rank.append(r2)\n","      model_dict[model] = (train_time, predict_time,\n","                          explained_variance_score(y_test, y_pred), \n","                          mean_absolute_error(y_test, y_pred),\n","                          r2_score(y_test, y_pred))\n","\n","  # Encontrando o modelo com maior R2\n","  max_r2 = max(r2_rank)\n","  max_index = r2_rank.index(max_r2)\n","\n","  # Registro dos parâmetros estatísticos obtidos no melhor modelo\n","  model_stats = {}\n","  model_stats[model_rank[max_index]] = model_dict[model_rank[max_index]]\n","\n","  return model_rank[max_index], model_stats\n","\n","def make_data_model_format(df):\n","\n","    # Removendo o valor .0 na coluna ordem para facilitar a remoção\n","    df['ordem'] = df['ordem'].str.replace('.0', '')\n","\n","    # Removendo os valores de ordem = 1 e 2\n","    # 1 = Dados de Laudo incompletos\n","    # 2 = Dados de S.A Farinha\n","    filtro1 = df['ordem'] != '1'\n","    filtro2 = df['ordem'] != '2'\n","    df = (df.loc[filtro1 & filtro2])\n","\n","    # Preenchendo id_massa nulos\n","    df['id_massa'].fillna(method=\"bfill\", inplace=True)\n","\n","    # Alterando o tipo dos dados da coluna valor\n","    df['valor'] = df['valor'].astype('float32')\n","\n","    # Gerando novo df com os dados de etapa pivotados\n","    reformated_data = pd.pivot_table(df,values='valor',index=['id_iteracao', 'id_massa'], columns=['etapa']).reset_index()\n","\n","    colunas = list(reformated_data.drop(['id_iteracao','id_massa'], axis=1).columns)\n","\n","    fill_values(colunas, reformated_data)\n","\n","    return reformated_data\n","\n","def find_better_model(df_model, X, y):\n","  '''Função para encontrar modelo com melhor R2 score'''\n","\n","  # Separação dos dados de treino e teste\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","  # Aplicando StandardScaler nos dados de treino e teste\n","  sc = StandardScaler()\n","  sc.fit(X_train)\n","  \n","  # Transformando os dados conforme StandardScaler obtido\n","  x_train_scaled = sc.transform(X_train)\n","  x_test_scaled = sc.transform(X_test)\n","\n","  # Definindo dos modelos que serão testados\n","  regressors = [\n","  LinearRegression(),\n","  Ridge(),\n","  RandomForestRegressor(),\n","  DecisionTreeRegressor()\n","  ]\n","\n","  # Obtendo o melhor modelo testado\n","  best_testing_models, model_stats = compare_models(x_train_scaled, x_test_scaled, y_train, y_test, regressors)\n","  \n","  # Prints com as estatísticas do modelo com maior R2\n","  print(\"############################\")\n","  print(f\"O modelo que obteve melhor performance foi: {best_testing_models}\\n\")\n","  print(f\"\\t- Training time: {round(model_stats[best_testing_models][0], 3)}s\")\n","  print(f\"\\t- Prediction time: {round(model_stats[best_testing_models][1],3)}s\")\n","  print(f\"\\t- Explained variance: {round(model_stats[best_testing_models][2], 3)}\")\n","  print(f\"\\t- Mean absolute error: {round(model_stats[best_testing_models][3],3)}\")\n","  print(f\"\\t- R2 score: {round(model_stats[best_testing_models][4], 3)}\")\n","  print(\"############################\\n\")\n","  \n","  return best_testing_models, model_stats\n","\n","def make_model(df_model):\n","  '''Função para criar modelo de previsão a partir do melhor R2 score obtido'''\n","\n","  # Eliminando as colunas não utilisadas no modelo\n","  modeling_data = df_model.drop(columns=['id_iteracao', 'id_massa'])\n","\n","  # Separação de features e variável target\n","  colunas = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', \n","            'quantidade_de_fermento', 'tempo_de_batimento_1ª_fase', 'tempo_de_batimento_2ª_fase', 'temperatura_de_massa',\n","            'peso_cru', 'altura_da_faca','pressão_rolo_de_borracha_direito', 'pressão_rolo_de_borracha_esquerdo','velocidade_da_moldadora',\n","            'velocidade_forçador', 'velocidade_lona_moldadora', 'pressão_rolo_estriado',\n","            'tempo_de_cozimento', 'pressão_do_lastro_-_zona_1', 'pressão_do_lastro_-_zona_2', 'pressão_do_lastro_-_zona_3', 'pressão_do_lastro_-_zona_4',\n","            'pressão_do_teto_-_zona_1', 'pressão_do_teto_-_zona_2', 'pressão_do_teto_-_zona_3', 'pressão_do_teto_-_zona_4',\n","            'temperatura_zona_1', 'temperatura_zona_2', 'temperatura_zona_3', 'temperatura_zona_4',\n","            'comprimento','largura', 'peso_assado', 'umidade_(água)', 'ph', 'cor_hunter_l_-_teto']\n","\n","  X = modeling_data.loc[:,colunas] #Definindo colunas de interesse e ordenação necessária\n","  y = modeling_data[['expansão']]\n","  \n","  best_testing_models, model_stats = find_better_model(df_model, X, y)\n","\n","  # Aplicando StandardScaler nos dados de treino e teste\n","  sc_fit = StandardScaler()\n","  sc_fit.fit(X)\n","  \n","  # Transformando os dados conforme StandardScaler obtido\n","  X_scaled = sc_fit.transform(X)\n","\n","  # Treinando o melhor modelo obtido com toda a base de dados\n","  the_best_model = best_testing_models.fit(X_scaled, y)\n","\n","  return the_best_model, sc_fit, model_stats\n","\n","def make_lr_model(df_model):\n","  '''Função para criar modelo de Regressão Linear para otimização dos parâmetros'''\n","\n","  # Eliminando as colunas não utilisadas no modelo\n","  modeling_data = df_model.drop(columns=['id_iteracao', 'id_massa'])\n","\n","  # Separação de features e variável target\n","  colunas = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', \n","            'quantidade_de_fermento', 'tempo_de_batimento_1ª_fase', 'tempo_de_batimento_2ª_fase', 'temperatura_de_massa',\n","            'peso_cru', 'altura_da_faca','pressão_rolo_de_borracha_direito', 'pressão_rolo_de_borracha_esquerdo','velocidade_da_moldadora',\n","            'velocidade_forçador', 'velocidade_lona_moldadora', 'pressão_rolo_estriado',\n","            'tempo_de_cozimento', 'pressão_do_lastro_-_zona_1', 'pressão_do_lastro_-_zona_2', 'pressão_do_lastro_-_zona_3', 'pressão_do_lastro_-_zona_4',\n","            'pressão_do_teto_-_zona_1', 'pressão_do_teto_-_zona_2', 'pressão_do_teto_-_zona_3', 'pressão_do_teto_-_zona_4',\n","            'temperatura_zona_1', 'temperatura_zona_2', 'temperatura_zona_3', 'temperatura_zona_4',\n","            'comprimento', 'largura', \"expansão\", 'umidade_(água)']\n","\n","  X = modeling_data.loc[:,colunas] #Definindo colunas de interesse e ordenação necessária\n","  y = modeling_data[['peso_assado']]\n","\n","  # Obtendo StandardScaler nos dados em X\n","  sc_linear_fit = StandardScaler()\n","  sc_linear_fit.fit(X)\n","  \n","  # Transformando os dados conforme StandardScaler obtido\n","  X_scaled = sc_linear_fit.transform(X)\n","\n","  # Treinando um modelo de Regressão Linear com toda a base de dados\n","  lr = LinearRegression()\n","  linear_regression_model = lr.fit(X_scaled, y)\n"," \n","  return linear_regression_model, sc_linear_fit\n","\n","def create_model_pickle(prediction_model=[], linear_model=[], model_stats=dict()):\n","    # Cria um objeto iterador com permissão de escrita - model.pkl\n","    ## Primeiro objeto é o melhor modelo obtido e StandardScale obtido\n","    ## Segundo objeto é o modelo de Linear Regression e StandardScale obtido\n","    ## Terceiro objeto são as estatísticas de performance do melhor modelo obtido\n","    with open('/content/drive/MyDrive/DNC_PredicaoExpansao/pkl_models/model_pkl', 'wb') as model_files:\n","        pkl.dump((prediction_model[0], linear_model[0], model_stats), model_files)\n","    print(\"Modelo para predição criado com sucesso !\")\n","\n","    with open('/content/drive/MyDrive/DNC_PredicaoExpansao/pkl_models/sc_pkl', 'wb') as sc_files:\n","        pkl.dump((prediction_model[1], linear_model[1]), sc_files)\n","    print(\"Standard Scale criado com sucesso !\")\n","\n","\n","def load_data_model(data='df_iteracoes.csv'):\n","    ## Criar condicional que verifica se o arquivo df_iteracoes.csv existe na pasta corrente\n","\n","    if data == \"/content/drive/MyDrive/DNC_PredicaoExpansao/Input/df_iteracoes.csv\" :\n","      # Apontando para a pasta do dataset, conforme padrão do projeto\n","      dataset_model = \"/content/drive/MyDrive/DNC_PredicaoExpansao/Input/df_iteracoes.csv\"\n","\n","      # Carregando os dados\n","      df = pd.read_csv(data, index_col='Unnamed: 0')\n","      return df\n","    else:\n","      print(\"Verifique se o arquivo: df_iteracoes.csv está salvo corretamente na pasta ../Input\")"]},{"cell_type":"markdown","metadata":{"id":"WiaBMEfSbhPA"},"source":["---"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3764,"status":"ok","timestamp":1672879319908,"user":{"displayName":"Eduardo Zanghi","userId":"15870388057949004930"},"user_tz":180},"id":"o-Ryi2rQpN5L","outputId":"70904f5e-afe8-4d1b-b5f9-cb1630fb2e1e","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Funções carregadas! Etapa 2 executada com sucesso!\n"]}],"source":["#@title 2 - Carregar funções para processar novo modelo preditor. Pressione [ ▶ ] abaixo\n","\n","try:\n","  file_name = \"dados.xlsx\"\n","\n","  if file_name not in os.listdir():\n","    uploaded = files.upload()\n","    for fn in uploaded.keys():\n","      print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","          name=fn, length=len(uploaded[fn])))  \n","except NameError:\n","  print(\"Verifique se a etapa 1 foi executada corretamente.\")\n","\n","else:\n","  try:\n","    df_processo, df_laudo = splitData(dataPreparation(carregaDados(file_name)))\n","    df_processo = dataCleaningProcess(df_processo)\n","    df_laudo = dataCleaningLaudo(df_laudo)\n","\n","    df_feature = prepareDataFeature(df_processo, df_laudo)\n","    df_target = prepareDataTarget(df_processo)\n","    df_etapa = prepareEtapa()\n","    df_laudo = prepareLaudo(df_laudo, df_target)\n","\n","    print(\"Funções carregadas! Etapa 2 executada com sucesso!\")\n","\n","  except NameError:\n","    print(\"Verifique se a etapa 1 foi executada corretamente.\")\n","  except:\n","    print(\"Verifique se o arquivo 'dados.xlsx' foi carregado e execute novamente.\")"]},{"cell_type":"markdown","metadata":{"id":"UVIXS3AidY0C"},"source":["---"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"e0Z4cFfIgi1m","outputId":"421868b0-3be0-445f-de46-61c0660725ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Verifique se as etapas 1 e 2 foram executadas corretamente.\n","Se necessário, vá em: \n","\t>>'Ambiente de execução' >> 'Reiniciar ambiente de execução' OU pressione Ctrl+M \n"]}],"source":["#@title 3 - Gerar novo modelo preditor. Pressione [ ▶ ] abaixo\n","\n","try:\n","  # # gerar o df_sorted com a ordenação de cada iteração\n","  df_sorted = get_ids(df_feature, df_etapa, df_target, df_laudo)\n","\n","  df_sorted = read_csv(\"/content/drive/MyDrive/DNC_PredicaoExpansao/Input/df_iteracoes.csv\")                                  # Apontar o caminho para /Input/df_iteracoes.csv\n","\n","  # # Salvando df_sorted em um arquivo externo\n","  df_sorted.to_csv(\"/content/drive/MyDrive/DNC_PredicaoExpansao/Input/df_iteracoes.csv\")                                      # Apontar o caminho para /Input/df_iteracoes.csv\n","  df_sorted.drop('Unnamed: 0', axis=1).to_excel(\"/content/drive/MyDrive/DNC_PredicaoExpansao/Output/dados_por_amostra.xlsx\")  # Apontar o caminho para /Output/dados_por_amostra.xlsx\n","\n","  # Gerando o modelo\n","  modelfile = \"/content/drive/MyDrive/DNC_PredicaoExpansao/Input/df_iteracoes.csv\"                                              # Apontar o caminho para /Input/df_iteracoes.csv\n","  model_data = load_data_model(modelfile)\n","  df_model = make_data_model_format(model_data)\n","  best_model, sc, model_stats = make_model(df_model)\n","  linear_regression_model, sc_lr = make_lr_model(df_model)\n","  create_model_pickle([best_model, sc], [linear_regression_model, sc_lr], model_stats)\n","\n","except NameError:\n","  print(\"Verifique se as etapas 1 e 2 foram executadas corretamente.\")\n","  print(\"Se necessário, vá em: \\n\\t>>'Ambiente de execução' >> 'Reiniciar ambiente de execução' OU pressione Ctrl+M \")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"},"vscode":{"interpreter":{"hash":"c727de62e0b83fb1a6b2ee1e99dd0f80c69b458145cebcf02e6a2c16a46b98b1"}}},"nbformat":4,"nbformat_minor":0}
