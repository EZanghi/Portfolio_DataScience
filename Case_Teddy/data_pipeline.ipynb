{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Teddy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Este Notebook tem por objetivo:</p>\n",
    "\n",
    "- Coletar os dados da API e carrega-los dentro de um Data Lake do banco de dados armazenado no Postgres;\n",
    "\n",
    "- Somente as linhas de dados completas devem ser armazenadas no Data Lake, linhas incompletas devem ser excluídas;\n",
    "\n",
    "- Criar string de conexão da engine;\n",
    "\n",
    "- Criar o script em SQL para a tabela\n",
    "\n",
    "<p> Para o fluxo a seguir subentede-se que o BD já possue toda estrutura proposta criada sendo ela:\n",
    "\n",
    "- Schema: teddy_360\n",
    "    - Tabela: teddy_360\n",
    "\n",
    "! Os scripts para criação do Schema e tabelas encontram-se na pasta deste projeto como *create_database.sql*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descomente as linhas [!pip install ...] caso precise instalar algumas das bibliotecas utilizadas\n",
    "\n",
    "## Instalar pacotes do requests\n",
    "# !pip install -q requests\n",
    "\n",
    "## Instalar pacotes do requests\n",
    "# !pip install -q pandas\n",
    "\n",
    "## Instalar Watermark\n",
    "# !pip install -q watermark\n",
    "\n",
    "## Instalar o psycopg2-binary -- para suprir dependência da lib sqlalchemy\n",
    "# !pip install psycopg2-binary -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas utilizadas\n",
    "\n",
    "import sqlalchemy as db         # Acessar e gerenciar BD\n",
    "import requests                 # Obter dados API - WEB\n",
    "import pandas as pd             # Manipular dados\n",
    "from datetime import datetime   # Manipular Data/hora\n",
    "\n",
    "# *Caso não seja possível importar alguma das bibliotecas acima,\n",
    "#  descomente a linha correspondente na célula acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlalchemy: 2.0.30\n",
      "pandas    : 2.2.2\n",
      "requests  : 2.32.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#versões dos pacotes usados neste notebook\n",
    "%reload_ext watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VERSÃO DAS LIB'S UTILIZADAS**\n",
    "- sqlalchemy: 2.0.30\n",
    "- pandas    : 2.2.2\n",
    "- requests  : 2.32.2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando as conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo variáveis de conexão\n",
    "\n",
    "## Fonte de dados\n",
    "api_data = 'https://jsonplaceholder.typicode.com/todos/'\n",
    "\n",
    "## Conexão BD\n",
    "db_url = db.URL.create(\n",
    "    \"postgresql\",\n",
    "    username=\"postgres\",\n",
    "    password=\"Password123\",\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"datalake\",\n",
    "    )\n",
    "\n",
    "# Criando conexão com BD Postgres\n",
    "engine = db.create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando dados da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset baixado possue:\n",
      " Linhas:  200 \n",
      " Colunas:  5 ['userId', 'id', 'title', 'completed', 'datetime_update']\n"
     ]
    }
   ],
   "source": [
    "# Obtendo os dados da API\n",
    "response = requests.get(api_data)\n",
    "data = response.json()\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# *** EXTRA ***\n",
    "# AQUI SERÁ INCLUÍDO A DATA/HORA A CADA LINHA DE DADO A FIM DE MELHORAR A QUALIDADE DOS DADOS\n",
    "df.insert(4, column=\"datetime_update\", value=datetime.now())\n",
    "\n",
    "# Descrevendo dados baixados\n",
    "colunas = list(df.columns)\n",
    "print(\"O dataset baixado possue:\\n Linhas: \", df.shape[0],\n",
    "       \"\\n Colunas: \", df.shape[1], colunas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando dados baixados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>completed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>5</td>\n",
       "      <td>95</td>\n",
       "      <td>vel nihil et molestiae iusto assumenda nemo qu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  id                                              title  completed\n",
       "94       5  95  vel nihil et molestiae iusto assumenda nemo qu...       True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Amostra dos dados\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId  id     title  completed\n",
       "False   False  False  False        200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica dados nulos no dataset\n",
    "df.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA - Verificando valores existentes no BD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Esta etapa tem por objetivo prevenir que a tabela duplique dados em caso de atualizações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo valores atuais no BD\n",
    "with engine.connect() as conn:\n",
    "   df_local = conn.execute(db.text(\"SELECT * FROM teddy_360.teddy_360\")).fetchall()\n",
    "\n",
    "\n",
    "# Transformando valores em um dicionario para criação de Dataframe para comparação\n",
    "data_dict = [{'userId': userId, \n",
    "            'id': id, \n",
    "            'title':title, \n",
    "            'completed':completed, \n",
    "            'datetime_update':datetime_update} \n",
    "            for userId, id, title, completed, datetime_update in df_local]\n",
    "\n",
    "# Criando DF de referência com dados da BD\n",
    "df_local = pd.DataFrame(data_dict)\n",
    "\n",
    "# Concatenando valores dos dois DF\n",
    "df_updated = pd.concat([df_local, df], ignore_index=True)\n",
    "\n",
    "# Removendo duplicados considerando a coluna 'id' como chave primária para nesta tabela\n",
    "# caso os valores de um 'id' seja alterado preserva-se o último registro\n",
    "df_updated.drop_duplicates(subset=['id'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando dados para o Banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove valores nulos do dataset\n",
    "df_updated.dropna(inplace=True)\n",
    "\n",
    "# Envia dados para o BD \n",
    "## Foram selecionados apenas as colunas existentes na API até o momento para evitar falha na carga de dados\n",
    "## Caso sejam incluídos novas colunas na API as mesmas devem ser incluídas no BD e na linha abaixo\n",
    "\n",
    "colunas_api = ['userId', 'id', 'title', 'completed', \"datetime_update\"]     # Colunas mapeadas até o momento \n",
    "\n",
    "df_updated[colunas_api].to_sql('teddy_360', engine, schema='teddy_360', if_exists='replace', index=False)\n",
    "\n",
    "# Verificando se todos os dados foram gravados no BD\n",
    "with engine.connect() as conn:\n",
    "   query = conn.execute(db.text(\"SELECT * FROM teddy_360.teddy_360\")).fetchall()\n",
    "\n",
    "if len(query) == len(df):\n",
    "   print(f\"Todas as {len(query)} foram importadas com sucesso!\")\n",
    "elif len(query) > len(df):\n",
    "   print(f\"Foi incluida {len(query) - len(df)} linha(s)!\")\n",
    "else:\n",
    "   print(f\"Faltaram {abs(len(df) - len(query))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
